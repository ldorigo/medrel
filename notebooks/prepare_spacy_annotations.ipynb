{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodigy.components.db import connect\n",
    "\n",
    "def dataset_to_train_data(dataset):\n",
    "    db = connect()\n",
    "    examples = db.get_dataset(dataset)\n",
    "    train_data = []\n",
    "    for eg in examples:\n",
    "        if eg['answer'] == 'accept':\n",
    "            entities = [(span['start'], span['end'], span['label'])\n",
    "                        for span in eg['spans']]\n",
    "            train_data.append(\n",
    "                (\n",
    "                    eg['text'], \n",
    "                    {\n",
    "                        \"entities\": entities,\n",
    "                        \"dep\": \"\",      # dependency label\n",
    "                        \"head\": 0,        # offset of token head relative to token index\n",
    "                        \"tag\": \"\",      # part-of-speech tag\n",
    "                        \"orth\": \"\",     # verbatim text of the token\n",
    "                    }\n",
    "                ))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dataset_to_train_data(\"medical-signs-gold-eval\")\n",
    "de = dataset_to_train_data('FULL_SIGN_gold_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medical_signs_relation_indicators',\n",
       " 'medical_signs_disease_terms',\n",
       " 'medical-signs-gold-eval',\n",
       " 'classify_relevant_sentences',\n",
       " 'FULL_SIGN_gold_train',\n",
       " 'FULL_SIGN_binary_train',\n",
       " 'gold_standard_updated_review_eval']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_sci_md',disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(de,open(\"./data/annotate/spacy_gold_data_eval.json\",\"w+\"))\n",
    "json.dump(de,open(\"./data/annotate/spacy_gold_data_train.json\",\"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oxidative and nitrosative stress were greater in patients with sickle cell anemia compared with control patients, but the rate of vaso-occlusive crisis events in sickle cell anemia was not associated with the level of oxidative stress.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import spacy\n",
    "from prodigy.components.db import connect\n",
    "from prodigy.util import split_evals\n",
    "from spacy.gold import GoldCorpus, minibatch, biluo_tags_from_offsets, tags_to_entities\n",
    "\n",
    "\n",
    "def prodigy_to_spacy(nlp, dataset):\n",
    "    \"\"\"Create spaCy JSON training data from a Prodigy dataset.\n",
    "\n",
    "    See https://spacy.io/api/annotation#json-input.\n",
    "    \"\"\"\n",
    "    db = connect()\n",
    "    examples = db.get_dataset(dataset)\n",
    "\n",
    "    offsets = []\n",
    "    for eg in examples:\n",
    "        if eg['answer'] == 'accept':\n",
    "            entities = [(span['start'], span['end'], span['label'])\n",
    "                        for span in eg['spans']]\n",
    "            offsets.append((eg['text'], {'entities': entities}))\n",
    "\n",
    "    docs = docs_from_offsets(nlp, offsets)\n",
    "    trees = docs_to_trees(docs)\n",
    "    return trees\n",
    "\n",
    "\n",
    "def docs_from_offsets(nlp, gold):\n",
    "    \"\"\"Create a sequence of Docs from a sequence of text, entity-offsets pairs.\"\"\"\n",
    "    docs = []\n",
    "    for text, entities in gold:\n",
    "        doc = nlp(text)\n",
    "        entities = entities['entities']\n",
    "        tags = biluo_tags_from_offsets(doc, entities)\n",
    "        if entities:\n",
    "            for start, end, label in entities:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span:\n",
    "                    doc.ents = list(doc.ents) + [span]\n",
    "        if doc.ents:  # remove to return documents without entities too\n",
    "            docs.append((doc, tags))\n",
    "    return docs\n",
    "\n",
    "\n",
    "def docs_to_trees(docs):\n",
    "    \"\"\"Create spaCy JSON training data from a sequence of Docs.\"\"\"\n",
    "    doc_trees = []\n",
    "    for d, doc_tuple in enumerate(docs):\n",
    "        doc, tags = doc_tuple\n",
    "        try:\n",
    "            tags_to_entities(tags)\n",
    "        except AssertionError:\n",
    "            print('Dropping {}'.format(d))\n",
    "            continue\n",
    "        if not tags:\n",
    "            print('Dropping {}'.format(d))\n",
    "            continue\n",
    "        sentences = []\n",
    "        for s in doc.sents:\n",
    "            s_tokens = []\n",
    "            for t in s:\n",
    "                token_data = {\n",
    "                    'id': t.i,\n",
    "                    'orth': t.orth_,\n",
    "                    'tag': t.tag_,\n",
    "                    'head': t.head.i - t.i,\n",
    "                    'dep': t.dep_,\n",
    "                    'ner': tags[t.i],\n",
    "                }\n",
    "                s_tokens.append(token_data)\n",
    "            sentences.append({'tokens': s_tokens})\n",
    "        doc_trees.append({\n",
    "            'id': d,\n",
    "            'paragraphs': [\n",
    "                {\n",
    "                    'raw': doc.text,\n",
    "                    'sentences': sentences,\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    return doc_trees\n",
    "\n",
    "dev = prodigy_to_spacy(nlp, 'medical-signs-gold-eval')\n",
    "train = prodigy_to_spacy(nlp, 'FULL_SIGN_gold_train')\n",
    "\n",
    "\n",
    "with open('data/annotate/train.json', 'w+') as f:\n",
    "    json.dump(train, f)\n",
    "with open('data/annotate/dev.json', 'w+') as f:\n",
    "    json.dump(dev, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prodigy]",
   "language": "python",
   "name": "conda-env-prodigy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
